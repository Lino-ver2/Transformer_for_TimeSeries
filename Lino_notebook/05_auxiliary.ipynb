{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "parent_dir = 'Predict-Future-Sales'\n",
    "p_sub = sys.path[0]\n",
    "\n",
    "ride = ''\n",
    "for path in p_sub.split('/'):\n",
    "    if path != parent_dir:\n",
    "        ride = ride + path + '/'\n",
    "    else:\n",
    "        ride = ride + path + '/'\n",
    "        break\n",
    "sys.path[0] = ride\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from module.lino_module.preprocess import mode_of_freq, tde_dataset_wm\n",
    "from module.lino_module.inference import RecurrentInference\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.lino_module.model import TransformerModel\n",
    "\n",
    "class WithAuxiliary(nn.Module):\n",
    "    def __init__(self, d_model, head, device):\n",
    "        super(WithAuxiliary, self).__init__()\n",
    "        self.base = TransformerModel(d_model, head, device)\n",
    "        self.auxiliary = TransformerModel(d_model, head, device)\n",
    "\n",
    "    def forward(self, src, tgt, y=None):\n",
    "        base_pred = self.base(src, tgt)\n",
    "        auxiliary_pred = self.base(src, tgt)\n",
    "        if self.training:\n",
    "            auxiliary_label = y - base_pred.squeeze(2)\n",
    "            return base_pred, auxiliary_pred, auxiliary_label\n",
    "        else:\n",
    "            return base_pred, auxiliary_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithAuxiliary(nn.Module):\n",
    "    def __init__(self, base_func, auxiliary_func):\n",
    "        super(LossWithAuxiliary, self).__init__()\n",
    "        self.base_func = base_func\n",
    "        self.auxiliary_func = auxiliary_func\n",
    "\n",
    "    def forward(self, base_pred, auxiliary_pred, y, auxiliary_label):\n",
    "        base_loss = self.base_func(base_pred, y)\n",
    "        auxiliary_loss = self.auxiliary_func(auxiliary_pred, auxiliary_label)\n",
    "        loss = base_loss * 0.5 + auxiliary_loss * 0.5\n",
    "        return loss, base_loss, auxiliary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_auxiliary(model: object,\n",
    "                            train: DataLoader,\n",
    "                            test: DataLoader,\n",
    "                            device: torch.device,\n",
    "                            criterion: object,\n",
    "                            optimizer: object,\n",
    "                            epochs: int,\n",
    "                            verbose=10,\n",
    "                            center=80) -> Tuple[object, Tensor, Tensor]:\n",
    "    \"\"\"訓練用関数\"\"\"\n",
    "    loss_pack = {'train': {'loss': [], 'base': [], 'auxiliary': []},\n",
    "              'eval': {'loss': [], 'base': [], 'auxiliary': []},\n",
    "              'test': {'loss': [], 'base': [], 'auxiliary': []}}\n",
    "    test_loss = []\n",
    "    print(' start '.center(center, '-'))\n",
    "    start_point = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = {'train': {'loss': [], 'base': [], 'auxiliary': []},\n",
    "                      'eval': {'loss': [], 'base': [], 'auxiliary': []},\n",
    "                      'test': {'loss': [], 'base': [], 'auxiliary': []}}\n",
    "\n",
    "        cache = None\n",
    "        for i, pack in enumerate(train):\n",
    "            inputs = [content.to(device) for content in pack]\n",
    "            # モデル訓練\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                # キャッシュから１バッチ前のデータで訓練\n",
    "                traing_eval(model, optimizer, criterion, cache, epoch_loss, 'train')\n",
    "                # 勾配計算\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # モデル評価\n",
    "            traing_eval(model, optimizer, criterion, inputs, epoch_loss, 'eval')\n",
    "            # データをキャッシュに保存して次回の訓練データにする\n",
    "            cache = (src, tgt, y)\n",
    "\n",
    "        # テストデータによる評価\n",
    "        for pack in test:\n",
    "            src, tgt, y = [content.to(device) for content in pack]\n",
    "            traing_eval(model, optimizer, criterion, inputs, epoch_loss, 'test')\n",
    "\n",
    "        # 損失データの登録\n",
    "        appender(loss_pack, epoch_loss, 'train')\n",
    "        appender(loss_pack, epoch_loss, 'eval')\n",
    "        appender(loss_pack, epoch_loss, 'test')\n",
    "\n",
    "        # lossのログを表示\n",
    "        logger(verbose, epoch, center, epoch_loss)\n",
    "\n",
    "    print(' complete!! '.center(center, '-'))\n",
    "    print(f'Execution_time: {round(time.time() - start_point, 3)}')\n",
    "    return model, loss_pack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traing_eval(model, optimizer, criterion, inputs, epoch_loss, mode):\n",
    "    src, tgt, y = inputs\n",
    "    key = mode\n",
    "    if mode=='test':\n",
    "        mode = 'eval'\n",
    "    getattr(model, mode)()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    base, auxiliary, label = model(src, tgt, y)\n",
    "    base, auxiliary = base.squeeze(2), auxiliary.squeeze(2)\n",
    "    loss, base_loss, auxiliary_loss = criterion(base, auxiliary, y, label)\n",
    "    output_pack = (base, auxiliary)\n",
    "\n",
    "    epoch_loss[key]['loss'].append(loss.item())\n",
    "    epoch_loss[key]['base'].append(base.item())\n",
    "    epoch_loss[key]['auxiliary'].append(auxiliary.item())\n",
    "    return output_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appender(dic1, dic2, mode):\n",
    "    for key, value in dic2[mode].items():\n",
    "        dic1[mode][key].append(value)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(verbose, epoch, center, epoch_loss):\n",
    "    if verbose == 0:\n",
    "        return None\n",
    "    elif epoch % verbose == 0:\n",
    "        print(f' epoch_{epoch} '.center(center))\n",
    "        train_mean = torch.mean(torch.tensor(epoch_loss['train'])).item()\n",
    "        valid_mean = torch.mean(torch.tensor(epoch_loss['eval'])).item()\n",
    "        test_mean = torch.mean(torch.tensor(epoch_loss['test'])).item()\n",
    "        print('train_loss: ', round(train_mean, 4),\n",
    "                '| validation_loss: ', round(valid_mean, 4),\n",
    "                '| test_loss: ', round(test_mean, 4))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = '../../'\n",
    "data = pd.read_csv(parent_path + '/data/sales_train.csv')\n",
    "data = mode_of_freq(data).item_cnt_day\n",
    "demo = np.arange(len(data))\n",
    "ds = pd.Series(demo , index=data.index)\n",
    "\n",
    "d_model = 4\n",
    "dilation = 1\n",
    "\n",
    "kwrgs ={'data': ds,\n",
    "        'seq': 7,\n",
    "        'd_model': d_model,\n",
    "        'dilation': dilation,\n",
    "        'src_tgt_seq': (6, 3),\n",
    "        'step_num': 2,\n",
    "        'batch_size': 64,\n",
    "        'scaler': None,\n",
    "        'daily': False,\n",
    "        'weekly': False,\n",
    "        'weekly_num': False,\n",
    "        'monthly': False,\n",
    "        'train_rate': 0.9\n",
    "        }\n",
    "train, test = tde_dataset_wm(**kwrgs)\n",
    "src, tgt, y = next(iter(train))\n",
    "src.shape, tgt.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下途中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "self = WithAuxiliary(d_model=4, head=2, device=device)\n",
    "self.train()\n",
    "base_pred, auxiliary_pred, label = self(src, tgt, y)\n",
    "base_pred, auxiliary_pred = base_pred.squeeze(2), auxiliary_pred.squeeze(2)\n",
    "\n",
    "loss_with_auxiliary = LossWithAuxiliary(nn.MSELoss(), nn.MSELoss())\n",
    "loss, base_loss, auxiliary_loss = loss_with_auxiliary(base_pred, auxiliary_pred, y, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(24, dtype=torch.float).reshape(2, 3, 4)\n",
    "\n",
    "size = (3, 4)\n",
    "mask = torch.randn(size)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "norm = nn.LayerNorm(4)\n",
    "\n",
    "normed = x@mask.T\n",
    "atten_mask = sigmoid(normed)\n",
    "\n",
    "atten_mask\n",
    "\n",
    "\n",
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self, size, dim):\n",
    "        super(AttentionConv, self).__init__()\n",
    "        self.atten_weight = nn.rand(size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.BatchNorm2d(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = \n",
    "\n",
    "import copy\n",
    "\n",
    "class DifferPredictLayer(nn.Module):\n",
    "    def __init__(self, module, n):\n",
    "        super(DifferPredictLayer, self).__init__()\n",
    "        # self.hidden = clones(module, n)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return None\n",
    "        \n",
    "def clones(module, n): \n",
    "    return nn.Modulelist([copy.deepcopy(module) for _ in range(n)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f415b4ed372d607fec632355f4d17dd884dbf40d29d4f41bd390ce33285f7d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
