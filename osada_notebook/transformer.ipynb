{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lino/Desktop/Predict-Future-Sales',\n",
       " '/Users/lino/.vscode/extensions/ms-toolsai.jupyter-2022.11.1003412109/pythonFiles',\n",
       " '/Users/lino/.vscode/extensions/ms-toolsai.jupyter-2022.11.1003412109/pythonFiles/lib/python',\n",
       " '/Users/lino/opt/anaconda3/envs/datascience/lib/python39.zip',\n",
       " '/Users/lino/opt/anaconda3/envs/datascience/lib/python3.9',\n",
       " '/Users/lino/opt/anaconda3/envs/datascience/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/lino/opt/anaconda3/envs/datascience/lib/python3.9/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# Over Ride Parent Path.\n",
    "parent_dir_name = 'Predict-Future-Sales'\n",
    "sys_path = ''\n",
    "for p in str(sys.path[0]).split('/'):\n",
    "    if p != parent_dir_name:\n",
    "        sys_path = sys_path + p + '/'\n",
    "    else:\n",
    "        sys_path += parent_dir_name\n",
    "        break\n",
    "\n",
    "sys.path[0] = sys_path\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lino/opt/anaconda3/envs/datascience/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import LayerNorm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "# Mymodule\n",
    "from module.lino import mode_of_freq, making_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Transformer model.\n",
    "\n",
    "    Args:\n",
    "        d_model: encoder/decoder inputsの特徴量数\n",
    "        nhead: Multi-head Attentionのヘッド数\n",
    "        nhid: feedforward neural networkの次元数\n",
    "        nlayers: encoder内のsub-encoder-layerの数\n",
    "        dropout: ドロップアウト率\n",
    "        activation: 活性化関数\n",
    "        use_src_mask: encoderで時系列マスクを適用するか\n",
    "        cat_embs: 各カテゴリ変数におけるカテゴリ数とembedding次元数\n",
    "        fc_dims: decoder outputsに対するfeedforward neural networkの次元数\n",
    "        device: cpu or gpu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int = 8,\n",
    "        nhid: int = 2048,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        use_src_mask: bool = False,\n",
    "        fc_dims: Optional[List[int]] = None,\n",
    "        device: Optional[bool] = None,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # デバイスの選定\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        # アトリビュートの格納先\n",
    "        self.tgt_mask = None\n",
    "        self.src_mask = None\n",
    "        self.use_src_mask = use_src_mask\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model, nhead, nhid, dropout, activation\n",
    "        )\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            encoder_layers, nlayers, encoder_norm\n",
    "        )\n",
    "\n",
    "        decoder_layers = TransformerDecoderLayer(\n",
    "            d_model, nhead, nhid, dropout, activation\n",
    "        )\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.transformer_decoder = TransformerDecoder(\n",
    "            decoder_layers, nlayers, decoder_norm\n",
    "        )\n",
    "\n",
    "        if fc_dims is None:\n",
    "            fc_dims = []\n",
    "\n",
    "        if len(fc_dims) > 0:\n",
    "            fc_layers = []\n",
    "            for i, hdim in enumerate(fc_dims):\n",
    "                if i != 0:\n",
    "                    fc_layers.append(nn.Linear(fc_dims[i - 1], hdim))\n",
    "                    fc_layers.append(nn.Dropout(dropout))\n",
    "                else:\n",
    "                    fc_layers.append(nn.Linear(d_model, hdim))\n",
    "                    fc_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            self.fc = nn.Sequential(*fc_layers)\n",
    "            self.output = nn.Linear(fc_dims[-1], 1)\n",
    "        else:\n",
    "            self.fc = None\n",
    "            self.output = nn.Linear(d_model, 1)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"未来の情報を考慮しないためのマスクを生成.\"\"\"\n",
    "\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"パラメータを初期化.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Optional[Tensor] = None,\n",
    "        tgt: Optional[Tensor] = None,\n",
    "        memory: Optional[Tensor] = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Transformerを適用.\n",
    "\n",
    "        Args:\n",
    "            src: Encoder input（数値）\n",
    "            tgt: Decoder input（数値）\n",
    "            memory: Encoder output\n",
    "        \"\"\"\n",
    "\n",
    "        if src is not None:\n",
    "            src = Variable(src, requires_grad=True).to(self.device).float()\n",
    "            src = self.pos_encoder(src)\n",
    "\n",
    "            if self.use_src_mask:\n",
    "                if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                    mask = self._generate_square_subsequent_mask(len(src)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "                    self.src_mask = mask\n",
    "\n",
    "            memory = self.transformer_encoder(src, mask=self.src_mask)\n",
    "\n",
    "        if tgt is None:\n",
    "            return memory\n",
    "        else:\n",
    "            tgt = Variable(tgt, requires_grad=True).to(self.device).float()\n",
    "\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "\n",
    "            if self.tgt_mask is None or self.tgt_mask.size(0) != len(tgt):\n",
    "                mask = self._generate_square_subsequent_mask(len(tgt)).to(self.device)\n",
    "                self.tgt_mask = mask\n",
    "\n",
    "            decoder_output = self.transformer_decoder(\n",
    "                tgt, memory, tgt_mask=self.tgt_mask\n",
    "            )\n",
    "\n",
    "            fc_input = decoder_output\n",
    "\n",
    "            if self.fc is not None:\n",
    "                fc_output = self.fc(fc_input)\n",
    "            else:\n",
    "                fc_output = fc_input\n",
    "\n",
    "            output = self.output(fc_output)\n",
    "\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"PositionalEncodingを適用.\"\"\"\n",
    "\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "data = pd.read_csv('../data/sales_train.csv')\n",
    "data = mode_of_freq(data)\n",
    "pack = making_dataset(data.iloc[:, -1], span=32)\n",
    "x_train, x_test, y_train, y_test = [torch.from_numpy(trg.astype(np.float32)).clone() for trg in pack]\n",
    "\n",
    "batch_size = 100\n",
    "x_batch = (x_train[i*batch_size:(i+1)*batch_size] for i in range(1 + x_train.shape[0]//batch_size))\n",
    "y_batch = (y_train[i*batch_size:(i+1)*batch_size] for i in range(1 + len(y_train)//batch_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32\n",
    "transformer = TransformerModel(d_model)\n",
    "optimizer = Adam(transformer.parameters(), lr=1e-4, betas=[0.9, 0.98], eps=10e-9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- epoch_0 ---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lino/opt/anaconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 100, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (100) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mセル13 を /Users/lino/Desktop/Predict-Future-Sales/osada_notebook/transformer.ipynb\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lino/Desktop/Predict-Future-Sales/osada_notebook/transformer.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lino/Desktop/Predict-Future-Sales/osada_notebook/transformer.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m transformer(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lino/Desktop/Predict-Future-Sales/osada_notebook/transformer.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lino/Desktop/Predict-Future-Sales/osada_notebook/transformer.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lino/Desktop/Predict-Future-Sales/osada_notebook/transformer.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 勾配計算\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/loss.py:530\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/functional.py:3279\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3276\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3277\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3279\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3280\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.9/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (100) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for i in range(epoch):\n",
    "    print(f' epoch_{i} '.center(50, '-'))\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        # モデル訓練\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(x)\n",
    "        loss = criterion(output, y)\n",
    "        train_loss.append(loss)\n",
    "        # 勾配計算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(1, 512)\n",
    "memory = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "tgt = torch.rand(4, 512)\n",
    "out = transformer_decoder(tgt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "596b88989fc0dc1fed1e4e461c9c9f08188a37ef0bdca6efc263739311be1bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
